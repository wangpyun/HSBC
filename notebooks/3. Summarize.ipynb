{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "import json\n",
    "import multiprocessing\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from utils import generate_response, get_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load demonstration data\n",
    "demo_file = '/Users/Pengyun_Wang/Projects/HSBC/data/demonstration.json'\n",
    "with open(demo_file, 'r') as f:\n",
    "    demo_data = json.load(f)\n",
    "\n",
    "# load web data\n",
    "web_file = '/Users/Pengyun_Wang/Projects/HSBC/data/popular_web_data.json'\n",
    "with open(web_file, 'r') as f:\n",
    "    web_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construct_prompt(demo_data, query):\n",
    "    \"\"\"\n",
    "    Construct a prompt for the model to generate a human-readable summary of the website structure.\n",
    "    Few shot learning is used to provide the model with context about the task.\n",
    "    \n",
    "    \"\"\"\n",
    "    start_phrase = f'''\n",
    "    Task: Use the following examples to generate a human-readable summary of the website structure. Make sure to follow the examples and provide a step by step descritption. Most importantly, make sure to provide a stand-alone overall summary as the last paragraph.\n",
    "    Prompt:\n",
    "    Summarize the DOM data in human-readable way:\n",
    "    '''\n",
    "    # add demo\n",
    "    for _, val in demo_data.items():\n",
    "        start_phrase +=f\"DOM: {val['web_data']}\\n\"\n",
    "        start_phrase +=f\"Summary: {val['summary']}\\n\"\n",
    "    # add query\n",
    "    start_phrase += f\"DOM: {query}\\n\"\n",
    "    start_phrase += f\"Summary:\"\n",
    "\n",
    "    return start_phrase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 10191 tokens (9191 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 10074 tokens (9074 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 9206 tokens (8206 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 10997 tokens (9997 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 11139 tokens (10139 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 11240 tokens (10240 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 10069 tokens (9069 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 10997 tokens (9997 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 16333 tokens (15333 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 13093 tokens (12093 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 9492 tokens (8492 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 10069 tokens (9069 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 8381 tokens (7381 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 14535 tokens (13535 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 10191 tokens (9191 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 11247 tokens (10247 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 9894 tokens (8894 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 13873 tokens (12873 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 9818 tokens (8818 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 9817 tokens (8817 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 9822 tokens (8822 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 17008 tokens (16008 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 10258 tokens (9258 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 8640 tokens (7640 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 9813 tokens (8813 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 8310 tokens (7310 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 11139 tokens (10139 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 9914 tokens (8914 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 12154 tokens (11154 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 10074 tokens (9074 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 16220 tokens (15220 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 8437 tokens (7437 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 8332 tokens (7332 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 8193 tokens, however you requested 15617 tokens (14617 in your prompt; 1000 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "# generate openai response with multiprocesses\n",
    "prompts = [construct_prompt(demo_data, web_data[key]) for key in web_data.keys()]\n",
    "\n",
    "with multiprocessing.Pool() as pool:\n",
    "    responses = pool.map(generate_response, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses saved to /Users/Pengyun_Wang/Projects/HSBC/data/openai_responses.json\n",
      "Number of successful responses: 850, Number of failed responses: 34\n"
     ]
    }
   ],
   "source": [
    "# save responses\n",
    "output = {}\n",
    "for i, key in enumerate(web_data.keys()):\n",
    "    if responses[i] is not None:\n",
    "        output[key] = responses[i]\n",
    "\n",
    "with open('/Users/Pengyun_Wang/Projects/HSBC/data/openai_responses.json', 'w') as f:\n",
    "    json.dump(output, f)\n",
    "    \n",
    "print(f\"Number of successful responses: {len(output)}, Number of failed responses: {len(prompts) - len(output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successful overall summaries: 843, Number of failed overall summaries: 7\n"
     ]
    }
   ],
   "source": [
    "# save the overall summary part in the response\n",
    "\n",
    "    \n",
    "overall_summary = {}\n",
    "for key, val in output.items():\n",
    "    tmp = get_summary(val)\n",
    "    if tmp:\n",
    "        overall_summary[key] = tmp\n",
    "    \n",
    "with open('/Users/Pengyun_Wang/Projects/HSBC/data/overall_summary.json', 'w') as f:\n",
    "    json.dump(overall_summary, f)\n",
    "\n",
    "print(f\"Number of successful overall summaries: {len(overall_summary)}, Number of failed overall summaries: {len(output) - len(overall_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HSBC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
